{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94a8233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47791f9f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43e00d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbb4ea",
   "metadata": {},
   "source": [
    "### Define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0e75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "#\n",
    "# Relevant directories are read from the config file:\n",
    "# dir_data:    full path to hdfs directory where the raw data .gz files are stored\n",
    "# dir_parquet: full path to hdfs directory where the parquet tables will be stored\n",
    "\n",
    "cf = ConfigParser()\n",
    "cf.read(\"config.cf\")\n",
    "\n",
    "dir_data = Path(cf.get(\"spark\", \"dir_data\"))\n",
    "dir_parquet = Path(cf.get(\"spark\", \"dir_parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78c5547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration hdfs\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "hdfs_dir_parquet = spark._jvm.org.apache.hadoop.fs.Path(dir_parquet.as_posix())\n",
    "hdfs_dir_data = spark._jvm.org.apache.hadoop.fs.Path(dir_data.as_posix())\n",
    "\n",
    "# Create output directories if they do not exist\n",
    "if not fs.exists(hdfs_dir_parquet):\n",
    "    fs.mkdirs(hdfs_dir_parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd6c10",
   "metadata": {},
   "source": [
    "### Save all tables to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2392d4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 23:30:47 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.util.ConcurrentModificationException\n",
      "\tat java.util.Hashtable$Enumerator.next(Hashtable.java:1387)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\n",
      "\tat scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\n",
      "\tat scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls201 - Rows: 114690034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls202 - Rows: 94685759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 23:34:05 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.util.ConcurrentModificationException\n",
      "\tat java.util.Hashtable$Enumerator.next(Hashtable.java:1387)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\n",
      "\tat scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\n",
      "\tat scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls203 - Rows: 72641357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls204 - Rows: 45965380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls205 - Rows: 4158216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls206 - Rows: 81582243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls207 - Rows: 298723394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls209 - Rows: 292526954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls210 - Rows: 25980433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls211 - Rows: 135250328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls212 - Rows: 425171934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls214 - Rows: 31141529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls215 - Rows: 732932899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls216 - Rows: 4291778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls222 - Rows: 360735009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls223 - Rows: 39008342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls224 - Rows: 326003568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls225 - Rows: 146158190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls226 - Rows: 98771304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls227 - Rows: 387970770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls228 - Rows: 219643872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls229 - Rows: 133578287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls230 - Rows: 133674867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed table tls231 - Rows: 357150176\n",
      "Processed table tls801 - Rows: 242\n",
      "Processed table tls803 - Rows: 3962\n",
      "Processed table tls901 - Rows: 764\n",
      "Processed table tls902 - Rows: 850\n",
      "Processed table tls904 - Rows: 2056\n",
      "CPU times: user 3.18 s, sys: 687 ms, total: 3.87 s\n",
      "Wall time: 44min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tables = [el.getPath().getName() for el in fs.listStatus(hdfs_dir_data)]\n",
    "tables = list(set([el.split(\"_\")[0] for el in tables if \n",
    "              (el.endswith(\".gz\") or el.endswith(\".csv\"))]))\n",
    "\n",
    "for tbl in sorted(tables):\n",
    "    # Read all gz of same table\n",
    "    df = spark.read.csv(f\"{dir_data.joinpath(tbl).as_posix()}*\", header=True)\n",
    "    # Save to parquet\n",
    "    df.write.parquet(\n",
    "        dir_parquet.joinpath(f\"{tbl}.parquet\").as_posix(),\n",
    "        mode=\"overwrite\",\n",
    "    )\n",
    "    print(\"Processed table\", tbl, \"- Rows:\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc23bae6",
   "metadata": {},
   "source": [
    "## Create a new table for Patent Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e329e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 00:27:43 ERROR scheduler.TaskSchedulerImpl: Lost executor 9 on node60.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:27:43 WARN scheduler.TaskSetManager: Lost task 26.0 in stage 160.0 (TID 8534) (node60.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:27:43 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 160.0 (TID 8524) (node60.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:27:43 WARN scheduler.TaskSetManager: Lost task 36.0 in stage 160.0 (TID 8544) (node60.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:27:43 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 160.0 (TID 8514) (node60.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:28:40 ERROR scheduler.TaskSchedulerImpl: Lost executor 0 on node10.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:28:40 WARN scheduler.TaskSetManager: Lost task 67.0 in stage 160.0 (TID 8579) (node10.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:28:40 WARN scheduler.TaskSetManager: Lost task 63.0 in stage 160.0 (TID 8575) (node10.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:28:40 WARN scheduler.TaskSetManager: Lost task 68.0 in stage 160.0 (TID 8580) (node10.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:28:40 WARN scheduler.TaskSetManager: Lost task 86.0 in stage 160.0 (TID 8598) (node10.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:06 ERROR scheduler.TaskSchedulerImpl: Lost executor 1 on node49.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:06 WARN scheduler.TaskSetManager: Lost task 76.0 in stage 160.0 (TID 8588) (node49.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:06 WARN scheduler.TaskSetManager: Lost task 99.0 in stage 160.0 (TID 8615) (node49.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:06 WARN scheduler.TaskSetManager: Lost task 107.0 in stage 160.0 (TID 8623) (node49.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:06 WARN scheduler.TaskSetManager: Lost task 106.0 in stage 160.0 (TID 8622) (node49.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:17 ERROR scheduler.TaskSchedulerImpl: Lost executor 5 on node36.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:17 WARN scheduler.TaskSetManager: Lost task 99.1 in stage 160.0 (TID 8630) (node36.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:17 WARN scheduler.TaskSetManager: Lost task 82.0 in stage 160.0 (TID 8594) (node36.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:17 WARN scheduler.TaskSetManager: Lost task 101.0 in stage 160.0 (TID 8617) (node36.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:17 WARN scheduler.TaskSetManager: Lost task 84.0 in stage 160.0 (TID 8596) (node36.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:19 ERROR scheduler.TaskSchedulerImpl: Lost executor 4 on node17.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:19 WARN scheduler.TaskSetManager: Lost task 96.0 in stage 160.0 (TID 8612) (node17.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:19 WARN scheduler.TaskSetManager: Lost task 113.0 in stage 160.0 (TID 8633) (node17.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:19 WARN scheduler.TaskSetManager: Lost task 110.0 in stage 160.0 (TID 8626) (node17.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:19 WARN scheduler.TaskSetManager: Lost task 97.0 in stage 160.0 (TID 8613) (node17.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 00:29:26 ERROR scheduler.TaskSchedulerImpl: Lost executor 7 on node56.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:26 WARN scheduler.TaskSetManager: Lost task 113.1 in stage 160.0 (TID 8648) (node56.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:26 WARN scheduler.TaskSetManager: Lost task 115.0 in stage 160.0 (TID 8635) (node56.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:26 WARN scheduler.TaskSetManager: Lost task 112.0 in stage 160.0 (TID 8632) (node56.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:29:26 WARN scheduler.TaskSetManager: Lost task 106.1 in stage 160.0 (TID 8628) (node56.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:30:18 ERROR scheduler.TaskSchedulerImpl: Lost executor 8 on node09.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:30:18 WARN scheduler.TaskSetManager: Lost task 164.0 in stage 160.0 (TID 8696) (node09.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:30:18 WARN scheduler.TaskSetManager: Lost task 140.0 in stage 160.0 (TID 8672) (node09.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:30:18 WARN scheduler.TaskSetManager: Lost task 152.0 in stage 160.0 (TID 8684) (node09.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 00:30:18 WARN scheduler.TaskSetManager: Lost task 156.0 in stage 160.0 (TID 8688) (node09.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "[Stage 164:===================================================> (193 + 7) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table tls904 - Rows: 114690034\n",
      "CPU times: user 394 ms, sys: 75.3 ms, total: 470 ms\n",
      "Wall time: 6min 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_201 = spark.read.parquet(dir_parquet.joinpath(f\"tls201.parquet\").as_posix())\n",
    "df_202 = spark.read.parquet(dir_parquet.joinpath(f\"tls202.parquet\").as_posix())\n",
    "df_203 = spark.read.parquet(dir_parquet.joinpath(f\"tls203.parquet\").as_posix())\n",
    "\n",
    "patstat_appln = (df_201.join(df_202, df_201.appln_id ==  df_202.appln_id, \"left\")\n",
    "                      .drop(df_202.appln_id)\n",
    "                      .join(df_203, df_201.appln_id ==  df_203.appln_id, \"left\")\n",
    "                      .drop(df_203.appln_id)\n",
    "                )\n",
    "\n",
    "patstat_appln.write.parquet(\n",
    "        dir_parquet.joinpath(f\"patstat_appln.parquet\").as_posix(),\n",
    "        mode=\"overwrite\",\n",
    "    )\n",
    "\n",
    "print(\"Created table patstat_appln\", \"- Rows:\", patstat_appln.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3762055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 01:07:24 ERROR scheduler.TaskSchedulerImpl: Lost executor 11 on node89.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 138.0 in stage 179.0 (TID 9784) (node89.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 99.0 in stage 179.0 (TID 9780) (node89.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 67.0 in stage 179.0 (TID 9753) (node89.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 89.0 in stage 179.0 (TID 9767) (node89.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 ERROR scheduler.TaskSchedulerImpl: Lost executor 14 on node30.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 179.0 (TID 9751) (node30.cluster.tsc.uc3m.es executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 124.0 in stage 179.0 (TID 9772) (node30.cluster.tsc.uc3m.es executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 128.0 in stage 179.0 (TID 9818) (node30.cluster.tsc.uc3m.es executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 127.0 in stage 179.0 (TID 9806) (node30.cluster.tsc.uc3m.es executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 ERROR scheduler.TaskSchedulerImpl: Lost executor 2 on node25.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 22.0 in stage 179.0 (TID 9757) (node25.cluster.tsc.uc3m.es executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 73.0 in stage 179.0 (TID 9805) (node25.cluster.tsc.uc3m.es executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 51.0 in stage 179.0 (TID 9801) (node25.cluster.tsc.uc3m.es executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 179.0 (TID 9750) (node25.cluster.tsc.uc3m.es executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 ERROR scheduler.TaskSchedulerImpl: Lost executor 13 on node89.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 173.0 in stage 179.0 (TID 9817) (node89.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 168.0 in stage 179.0 (TID 9810) (node89.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 144.0 in stage 179.0 (TID 9786) (node89.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:24 WARN scheduler.TaskSetManager: Lost task 96.0 in stage 179.0 (TID 9776) (node89.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:25 ERROR scheduler.TaskSchedulerImpl: Lost executor 10 on node22.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:25 WARN scheduler.TaskSetManager: Lost task 131.0 in stage 179.0 (TID 9804) (node22.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:25 WARN scheduler.TaskSetManager: Lost task 158.0 in stage 179.0 (TID 9839) (node22.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:25 WARN scheduler.TaskSetManager: Lost task 150.0 in stage 179.0 (TID 9824) (node22.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:25 WARN scheduler.TaskSetManager: Lost task 26.0 in stage 179.0 (TID 9761) (node22.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 01:07:28 ERROR scheduler.TaskSchedulerImpl: Lost executor 3 on node73.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:28 WARN scheduler.TaskSetManager: Lost task 89.1 in stage 179.0 (TID 9831) (node73.cluster.tsc.uc3m.es executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:28 WARN scheduler.TaskSetManager: Lost task 138.1 in stage 179.0 (TID 9834) (node73.cluster.tsc.uc3m.es executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:33 ERROR scheduler.TaskSchedulerImpl: Lost executor 15 on node79.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:33 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 179.0 (TID 9835) (node79.cluster.tsc.uc3m.es executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:33 WARN scheduler.TaskSetManager: Lost task 124.1 in stage 179.0 (TID 9829) (node79.cluster.tsc.uc3m.es executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:33 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 179.0 (TID 9838) (node79.cluster.tsc.uc3m.es executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:33 WARN scheduler.TaskSetManager: Lost task 67.1 in stage 179.0 (TID 9832) (node79.cluster.tsc.uc3m.es executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:37 ERROR scheduler.TaskSchedulerImpl: Lost executor 12 on node54.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:37 WARN scheduler.TaskSetManager: Lost task 127.1 in stage 179.0 (TID 9827) (node54.cluster.tsc.uc3m.es executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:37 WARN scheduler.TaskSetManager: Lost task 67.2 in stage 179.0 (TID 9911) (node54.cluster.tsc.uc3m.es executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:37 WARN scheduler.TaskSetManager: Lost task 6.1 in stage 179.0 (TID 9830) (node54.cluster.tsc.uc3m.es executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:37 WARN scheduler.TaskSetManager: Lost task 99.1 in stage 179.0 (TID 9833) (node54.cluster.tsc.uc3m.es executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:40 ERROR scheduler.TaskSchedulerImpl: Lost executor 6 on node87.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:40 WARN scheduler.TaskSetManager: Lost task 124.2 in stage 179.0 (TID 9913) (node87.cluster.tsc.uc3m.es executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:40 WARN scheduler.TaskSetManager: Lost task 99.2 in stage 179.0 (TID 9922) (node87.cluster.tsc.uc3m.es executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/02 01:07:40 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 179.0 (TID 9914) (node87.cluster.tsc.uc3m.es executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72641357"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patstat_appln = spark.read.parquet(dir_parquet.joinpath(f\"patstat_appln.parquet\").as_posix())\n",
    "patstat_appln = patstat_appln.filter(patstat_appln.appln_abstract.isNotNull())\n",
    "patstat_appln.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad7e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
